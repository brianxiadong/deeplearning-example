"""
TwoLayerNet å¿«é€Ÿè®­ç»ƒæ¼”ç¤º
å±•ç¤ºä¸»è¦åŠŸèƒ½ï¼Œè®­ç»ƒè½®æ•°è¾ƒå°‘ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
"""
import numpy as np
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dezero import Variable, TwoLayerNet, mean_squared_error, sigmoid_simple


def quick_regression_demo():
    """å¿«é€Ÿå›å½’æ¼”ç¤º"""
    print("ğŸ”¥ å¿«é€Ÿå›å½’æ¼”ç¤ºï¼šæ‹Ÿåˆsinå‡½æ•°")
    print("=" * 50)
    
    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    x = np.random.uniform(-np.pi, np.pi, (200, 1))
    y = np.sin(x) + 0.1 * np.random.randn(200, 1)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_x, test_x = Variable(x[:160]), Variable(x[160:])
    train_y, test_y = Variable(y[:160]), Variable(y[160:])
    
    print(f"ğŸ“Š æ•°æ®: è®­ç»ƒé›† {train_x.shape}, æµ‹è¯•é›† {test_x.shape}")
    
    # åˆ›å»ºæ¨¡å‹
    model = TwoLayerNet(hidden_size=20, out_size=1)
    
    # è®­ç»ƒå‚æ•°
    learning_rate = 0.02
    epochs = 300
    
    print(f"ğŸ§  æ¨¡å‹: éšè—å±‚=20, å­¦ä¹ ç‡={learning_rate}, è®­ç»ƒè½®æ•°={epochs}")
    print()
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("è½®æ•°     è®­ç»ƒæŸå¤±     æµ‹è¯•æŸå¤±")
    print("-" * 35)
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        pred = model(train_x)
        loss = mean_squared_error(train_y, pred)
        
        # åå‘ä¼ æ’­
        model.cleargrads()
        loss.backward()
        
        # å‚æ•°æ›´æ–°
        for param in model.params():
            if param.grad is not None:
                param.data -= learning_rate * param.grad.data
        
        # æ‰“å°è¿›åº¦
        if epoch % 50 == 0 or epoch == epochs - 1:
            test_pred = model(test_x)
            test_loss = mean_squared_error(test_y, test_pred)
            print(f"{epoch:4d}     {loss.data:.6f}     {test_loss.data:.6f}")
    
    # æœ€ç»ˆè¯„ä¼°
    final_pred = model(test_x)
    final_loss = mean_squared_error(test_y, final_pred)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆæµ‹è¯•æŸå¤±: {final_loss.data:.6f}")
    
    # è®¡ç®—ä¸€äº›ç®€å•çš„è¯„ä¼°æŒ‡æ ‡
    mse = final_loss.data
    rmse = np.sqrt(mse)
    print(f"ğŸ“ˆ RMSE: {rmse:.6f}")
    
    return model


def quick_classification_demo():
    """å¿«é€Ÿåˆ†ç±»æ¼”ç¤º"""
    print("\n" + "ğŸ¯ å¿«é€Ÿåˆ†ç±»æ¼”ç¤ºï¼šäºŒå…ƒåˆ†ç±»")
    print("=" * 50)
    
    # ç”Ÿæˆç®€å•çš„åˆ†ç±»æ•°æ®
    np.random.seed(42)
    
    # ç±»åˆ«0: ä¸­å¿ƒåœ¨(-1, -1)
    class_0 = np.random.multivariate_normal([-1, -1], [[0.5, 0], [0, 0.5]], 100)
    # ç±»åˆ«1: ä¸­å¿ƒåœ¨(1, 1)  
    class_1 = np.random.multivariate_normal([1, 1], [[0.5, 0], [0, 0.5]], 100)
    
    x = np.vstack([class_0, class_1])
    y = np.hstack([np.zeros(100), np.ones(100)]).reshape(-1, 1)
    
    # æ‰“ä¹±æ•°æ®
    indices = np.random.permutation(200)
    x, y = x[indices], y[indices]
    
    # åˆ’åˆ†æ•°æ®é›†
    train_x, test_x = Variable(x[:160]), Variable(x[160:])
    train_y, test_y = Variable(y[:160]), Variable(y[160:])
    
    print(f"ğŸ“Š æ•°æ®: è®­ç»ƒé›† {train_x.shape}, æµ‹è¯•é›† {test_x.shape}")
    print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {np.bincount(train_y.data.flatten().astype(int))}")
    
    # åˆ›å»ºæ¨¡å‹
    model = TwoLayerNet(hidden_size=10, out_size=1)
    
    # è®­ç»ƒå‚æ•°
    learning_rate = 0.1
    epochs = 200
    
    print(f"ğŸ§  æ¨¡å‹: éšè—å±‚=10, å­¦ä¹ ç‡={learning_rate}, è®­ç»ƒè½®æ•°={epochs}")
    print()
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("è½®æ•°     è®­ç»ƒæŸå¤±     è®­ç»ƒå‡†ç¡®ç‡   æµ‹è¯•å‡†ç¡®ç‡")
    print("-" * 45)
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        logits = model(train_x)
        pred = sigmoid_simple(logits)
        loss = mean_squared_error(pred, train_y)  # ä½¿ç”¨MSEä½œä¸ºæŸå¤±
        
        # åå‘ä¼ æ’­
        model.cleargrads()
        loss.backward()
        
        # å‚æ•°æ›´æ–°
        for param in model.params():
            if param.grad is not None:
                param.data -= learning_rate * param.grad.data
        
        # æ‰“å°è¿›åº¦
        if epoch % 40 == 0 or epoch == epochs - 1:
            # è®­ç»ƒå‡†ç¡®ç‡
            train_pred_binary = (pred.data > 0.5).astype(float)
            train_acc = np.mean(train_pred_binary == train_y.data)
            
            # æµ‹è¯•å‡†ç¡®ç‡
            test_logits = model(test_x)
            test_pred = sigmoid_simple(test_logits)
            test_pred_binary = (test_pred.data > 0.5).astype(float)
            test_acc = np.mean(test_pred_binary == test_y.data)
            
            print(f"{epoch:4d}     {loss.data:.6f}     {train_acc:.4f}       {test_acc:.4f}")
    
    print(f"\nâœ… åˆ†ç±»è®­ç»ƒå®Œæˆ! æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    
    return model


def parameter_comparison():
    """å‚æ•°æ•°é‡å¯¹æ¯”"""
    print("\n" + "âš™ï¸  å‚æ•°æ•°é‡å¯¹æ¯”")
    print("=" * 50)
    
    hidden_sizes = [5, 10, 20, 50]
    input_size = 3
    output_size = 1
    
    print("éšè—å±‚å¤§å°   å‚æ•°æ•°é‡   å‚æ•°åˆ†å¸ƒ")
    print("-" * 35)
    
    for hidden_size in hidden_sizes:
        model = TwoLayerNet(hidden_size, output_size)
        
        # åˆå§‹åŒ–æƒé‡ä»¥è®¡ç®—å‚æ•°æ•°é‡
        dummy_input = Variable(np.random.randn(1, input_size))
        _ = model(dummy_input)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = 0
        param_info = []
        
        for i, param in enumerate(model.params()):
            param_count = param.data.size
            total_params += param_count
            param_info.append(f"{param.name}:{param_count}")
        
        param_str = " + ".join(param_info)
        print(f"{hidden_size:8d}     {total_params:6d}     {param_str}")


def model_architecture_demo():
    """æ¨¡å‹ç»“æ„æ¼”ç¤º"""
    print("\n" + "ğŸ—ï¸  æ¨¡å‹ç»“æ„æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹
    model = TwoLayerNet(hidden_size=15, out_size=3)
    
    # åˆ›å»ºè¾“å…¥
    x = Variable(np.random.randn(2, 5), name='input')
    
    print(f"è¾“å…¥: {x.shape}")
    print("æ¨¡å‹ç»“æ„:")
    print("  è¾“å…¥å±‚  -> çº¿æ€§å±‚1 -> Sigmoid -> çº¿æ€§å±‚2 -> è¾“å‡º")
    print(f"    {x.shape[1]}     ->    {15}    ->   æ¿€æ´»   ->    {3}    -> {model(x).shape}")
    
    # æ˜¾ç¤ºå‚æ•°ä¿¡æ¯
    print(f"\nå‚æ•°è¯¦æƒ…:")
    for i, param in enumerate(model.params()):
        print(f"  {param.name}: {param.shape} (å…ƒç´ æ•°: {param.size})")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    y = model(x)
    print(f"\nå‰å‘ä¼ æ’­ç»“æœ: {y.shape}")
    print(f"è¾“å‡ºå€¼èŒƒå›´: [{y.data.min():.3f}, {y.data.max():.3f}]")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ‰ TwoLayerNet å¿«é€Ÿè®­ç»ƒæ¼”ç¤º")
    print("åŒ…å«å›å½’ã€åˆ†ç±»ã€å‚æ•°å¯¹æ¯”å’Œç»“æ„æ¼”ç¤º")
    print("=" * 60)
    
    # å›å½’æ¼”ç¤º
    regression_model = quick_regression_demo()
    
    # åˆ†ç±»æ¼”ç¤º
    classification_model = quick_classification_demo()
    
    # å‚æ•°å¯¹æ¯”
    parameter_comparison()
    
    # æ¨¡å‹ç»“æ„æ¼”ç¤º
    model_architecture_demo()
    
    print("\n" + "ğŸŠ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("TwoLayerNet å¯ä»¥æˆåŠŸå¤„ç†å›å½’å’Œåˆ†ç±»ä»»åŠ¡!")


if __name__ == '__main__':
    main() 