
# Simple Vision Transformer (Simple ViT) å®Œæ•´æ•™ç¨‹
æœ¬ç³»åˆ—æ•™ç¨‹çš„æºç æ¥æºäº[https://github.com/lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch)ï¼Œä»…ç”¨ä½œå­¦ä¹ ä½¿ç”¨ï¼Œæ„Ÿè°¢lucidrainsåˆ†äº«çš„é«˜è´¨é‡ä»£ç ã€‚
ä»¥ä¸‹æ˜¯æˆ‘æ€»ç»“çš„æ•™ç¨‹ä»£ç ï¼š[https://github.com/brianxiadong/deeplearning-example/tree/main/pytorch-examples/vit-pytorch](https://github.com/brianxiadong/deeplearning-example/tree/main/pytorch-examples/vit-pytorch)

## ğŸ“š ç›®å½•
1. [ç®€ä»‹](#ç®€ä»‹)
2. [ç†è®ºåŸºç¡€](#ç†è®ºåŸºç¡€)
3. [ä»£ç æ¶æ„è§£æ](#ä»£ç æ¶æ„è§£æ)
4. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
5. [æµ‹è¯•æµç¨‹](#æµ‹è¯•æµç¨‹)
6. [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
7. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
8. [å®è·µç»ƒä¹ ](#å®è·µç»ƒä¹ )
9. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
10. [è¿›é˜¶å­¦ä¹ ](#è¿›é˜¶å­¦ä¹ )

---

## ğŸš€ ç®€ä»‹

Simple Vision Transformer (SimpleViT) æ˜¯å¯¹æ ‡å‡† Vision Transformer çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå®ƒå»é™¤äº†ä¸€äº›å¤æ‚çš„ç»„ä»¶ï¼Œä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“ç†è§£å’Œå®ç°ã€‚è¿™ä¸ªæ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ç†è§£ SimpleViT çš„æ¯ä¸€ä¸ªç»†èŠ‚ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- ç†è§£ Vision Transformer çš„åŸºæœ¬æ¦‚å¿µ
- æŒæ¡ SimpleViT çš„æ¶æ„å’Œå®ç°
- å­¦ä¼šå¦‚ä½•æµ‹è¯•å’Œè°ƒè¯• ViT æ¨¡å‹
- äº†è§£å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å…³é”®æŠ€æœ¯

### ğŸ“‹ å‰ç½®çŸ¥è¯†
- Python åŸºç¡€ç¼–ç¨‹
- PyTorch åŸºç¡€çŸ¥è¯†
- æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ
- æ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬ç†è§£

---

## ğŸ§  ç†è®ºåŸºç¡€

### Vision Transformer æ ¸å¿ƒæ€æƒ³

Vision Transformer å°†å›¾åƒå¤„ç†é—®é¢˜è½¬åŒ–ä¸ºåºåˆ—å¤„ç†é—®é¢˜ï¼š

1. **å›¾åƒåˆ†å— (Patch Embedding)**
   - å°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„patches
   - æ¯ä¸ªpatchè¢«è§†ä¸ºä¸€ä¸ªtoken
   - çº¿æ€§æŠ•å½±åˆ°é«˜ç»´ç©ºé—´

2. **ä½ç½®ç¼–ç  (Positional Encoding)**
   - ä¸ºæ¯ä¸ªpatchæ·»åŠ ä½ç½®ä¿¡æ¯
   - ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£patchä¹‹é—´çš„ç©ºé—´å…³ç³»

3. **Transformer ç¼–ç å™¨**
   - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
   - å‰é¦ˆç½‘ç»œ
   - æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

4. **åˆ†ç±»å¤´ (Classification Head)**
   - å…¨å±€å¹³å‡æ± åŒ–æˆ–CLS token
   - çº¿æ€§åˆ†ç±»å™¨

### SimpleViT vs æ ‡å‡†ViT

| ç‰¹æ€§ | æ ‡å‡†ViT | SimpleViT |
|------|---------|-----------|
| ä½ç½®ç¼–ç  | å¯å­¦ä¹ å‚æ•° | å›ºå®šçš„2Dæ­£å¼¦ä½™å¼¦ç¼–ç  |
| å…¨å±€è¡¨ç¤º | CLS token | å…¨å±€å¹³å‡æ± åŒ– |
| Dropout | æœ‰ | æ—  |
| å¤æ‚åº¦ | è¾ƒé«˜ | è¾ƒä½ |

---

## ğŸ—ï¸ ä»£ç æ¶æ„è§£æ

### é¡¹ç›®ç»“æ„
```
vit_pytorch/
â”œâ”€â”€ simple_vit.py             # è¯¦ç»†æ³¨é‡Šçš„SimpleViTå®ç°
tutorial/
â”œâ”€â”€ 01_SimpleViT_Tutorial.md   # æœ¬æ•™ç¨‹æ–‡æ¡£
test/
â”œâ”€â”€ ch01/
    â”œâ”€â”€ 01-simple_vit.py      # åŸºç¡€æµ‹è¯•ç”¨ä¾‹
    â”œâ”€â”€ train_simple_vit.py   # è®­ç»ƒè„šæœ¬
    â””â”€â”€ inference_simple_vit.py # æ¨ç†è„šæœ¬
```

### æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

#### `vit_pytorch/simple_vit.py`
è¿™æ˜¯æˆ‘ä»¬çš„ä¸»è¦å®ç°æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- **å·¥å…·å‡½æ•°**ï¼š`pair()`, `posemb_sincos_2d()`
- **æ ¸å¿ƒç»„ä»¶**ï¼š`FeedForward`, `Attention`, `Transformer`, `SimpleViT`
- **è¯¦ç»†æ³¨é‡Š**ï¼šæ¯è¡Œä»£ç éƒ½æœ‰ä¸­æ–‡è§£é‡Š

#### `test/ch01/01-simple_vit.py`
è¿™æ˜¯åŸºç¡€æµ‹è¯•æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
- åŸºæœ¬åŠŸèƒ½æµ‹è¯•
- ä¸åŒè¾“å…¥å°ºå¯¸æµ‹è¯•
- æ¢¯åº¦è®¡ç®—æµ‹è¯•
- æ¨¡å‹ç»„ä»¶æµ‹è¯•

#### `test/ch01/train_simple_vit.py`
è¿™æ˜¯å®Œæ•´çš„è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«ï¼š
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- æ¨¡å‹åˆ›å»ºå’Œé…ç½®
- è®­ç»ƒå¾ªç¯å®ç°
- æ¨¡å‹ä¿å­˜å’ŒéªŒè¯

#### `test/ch01/inference_simple_vit.py`
è¿™æ˜¯æ¨ç†è„šæœ¬ï¼ŒåŒ…å«ï¼š
- æ¨¡å‹åŠ è½½
- å›¾åƒé¢„å¤„ç†
- å•å¼ /æ‰¹é‡é¢„æµ‹
- ç»“æœå¯è§†åŒ–

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. å·¥å…·å‡½æ•°

#### `pair(t)` å‡½æ•°
```python
def pair(t):
    return t if isinstance(t, tuple) else (t, t)
```

**ä½œç”¨**ï¼šå°†å•ä¸ªå€¼è½¬æ¢ä¸ºå…ƒç»„ï¼Œæ”¯æŒæ­£æ–¹å½¢å’ŒçŸ©å½¢å›¾åƒã€‚

**ç¤ºä¾‹**ï¼š
- `pair(224)` â†’ `(224, 224)`
- `pair((256, 128))` â†’ `(256, 128)`

#### `posemb_sincos_2d()` å‡½æ•°
```python
def posemb_sincos_2d(h, w, dim, temperature=10000, dtype=torch.float32):
    # ç”Ÿæˆ2Dæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing="ij")
    # ... è¯¦ç»†å®ç°è§æºç 
```

**ä½œç”¨**ï¼šç”Ÿæˆ2Dæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ï¼Œä¸éœ€è¦å­¦ä¹ å‚æ•°ã€‚

**æ•°å­¦åŸç†**ï¼š
- åŸºäº Transformer åŸè®ºæ–‡çš„ä½ç½®ç¼–ç 
- ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
- ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆå”¯ä¸€çš„ç¼–ç 

### 2. å‰é¦ˆç½‘ç»œ (FeedForward)

```python
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.LayerNorm(dim),           # å±‚å½’ä¸€åŒ–
            nn.Linear(dim, hidden_dim),   # å‡ç»´
            nn.GELU(),                   # æ¿€æ´»å‡½æ•°
            nn.Linear(hidden_dim, dim),   # é™ç»´
        )
```

**å…³é”®ç‚¹**ï¼š
- **å±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
- **GELUæ¿€æ´»**ï¼šæ¯”ReLUæ›´å¹³æ»‘ï¼Œæ€§èƒ½æ›´å¥½
- **å‡ç»´-é™ç»´**ï¼šå…¸å‹çš„MLpç»“æ„

### 3. å¤šå¤´è‡ªæ³¨æ„åŠ› (Attention)

```python
class Attention(nn.Module):
    def __init__(self, dim, heads=8, dim_head=64):
        super().__init__()
        inner_dim = dim_head * heads
        self.heads = heads
        self.scale = dim_head ** -0.5  # ç¼©æ”¾å› å­
        
        self.norm = nn.LayerNorm(dim)
        self.attend = nn.Softmax(dim=-1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Linear(inner_dim, dim, bias=False)
```

**æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æµç¨‹**ï¼š
1. **è¾“å…¥å½’ä¸€åŒ–**ï¼š`x = self.norm(x)`
2. **ç”ŸæˆQã€Kã€V**ï¼š`qkv = self.to_qkv(x).chunk(3, dim=-1)`
3. **é‡æ’å½¢çŠ¶**ï¼šæ”¯æŒå¤šå¤´å¤„ç†
4. **è®¡ç®—æ³¨æ„åŠ›**ï¼š`dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale`
5. **åº”ç”¨Softmax**ï¼š`attn = self.attend(dots)`
6. **åŠ æƒæ±‚å’Œ**ï¼š`out = torch.matmul(attn, v)`

**æ•°å­¦å…¬å¼**ï¼š
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

### 4. Transformer ç¼–ç å™¨

```python
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Attention(dim, heads=heads, dim_head=dim_head),
                FeedForward(dim, mlp_dim)
            ]))
    
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x    # æ³¨æ„åŠ› + æ®‹å·®è¿æ¥
            x = ff(x) + x      # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        return self.norm(x)
```

**å…³é”®è®¾è®¡**ï¼š
- **æ®‹å·®è¿æ¥**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **å±‚å½’ä¸€åŒ–**ï¼šåœ¨æ®‹å·®è¿æ¥ä¹‹åè¿›è¡Œ
- **æ·±åº¦å †å **ï¼šå¤šä¸ªç›¸åŒçš„å±‚æé«˜è¡¨è¾¾èƒ½åŠ›

### 5. SimpleViT ä¸»æ¨¡å‹

```python
class SimpleViT(nn.Module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64):
        super().__init__()
        
        # è®¡ç®—patchç›¸å…³å‚æ•°
        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)
        patch_dim = channels * patch_height * patch_width
        
        # Patch embedding
        self.to_patch_embedding = nn.Sequential(
            Rearrange("b c (h p1) (w p2) -> b (h w) (p1 p2 c)", p1=patch_height, p2=patch_width),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            nn.LayerNorm(dim),
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = posemb_sincos_2d(
            h=image_height // patch_height,
            w=image_width // patch_width,
            dim=dim,
        )
        
        # Transformerç¼–ç å™¨
        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)
        
        # åˆ†ç±»å¤´
        self.linear_head = nn.Linear(dim, num_classes)
```

**å‰å‘ä¼ æ’­æµç¨‹**ï¼š
1. **Patch Embedding**ï¼šå›¾åƒ â†’ patches â†’ åµŒå…¥å‘é‡
2. **æ·»åŠ ä½ç½®ç¼–ç **ï¼šè®©æ¨¡å‹ç†è§£ç©ºé—´å…³ç³»
3. **Transformerç¼–ç **ï¼šæå–ç‰¹å¾è¡¨ç¤º
4. **å…¨å±€æ± åŒ–**ï¼šå¹³å‡æ± åŒ–æ‰€æœ‰patchç‰¹å¾
5. **åˆ†ç±»é¢„æµ‹**ï¼šçº¿æ€§å±‚è¾“å‡ºåˆ†ç±»logits

---

## ğŸ§ª æµ‹è¯•æµç¨‹

### æµ‹è¯•ç”¨ä¾‹è®¾è®¡

æˆ‘ä»¬è®¾è®¡äº†4ä¸ªæµ‹è¯•å‡½æ•°æ¥éªŒè¯æ¨¡å‹çš„æ­£ç¡®æ€§ï¼š

#### 1. åŸºæœ¬åŠŸèƒ½æµ‹è¯• (`test_simple_vit_basic`)
```python
def test_simple_vit_basic():
    model = SimpleViT(
        image_size=224,
        patch_size=16,
        num_classes=1000,
        dim=512,
        depth=6,
        heads=8,
        mlp_dim=1024
    )
    
    input_tensor = torch.randn(2, 3, 224, 224)
    output = model(input_tensor)
    
    assert output.shape == (2, 1000)
    assert not torch.isnan(output).any()
```

**æµ‹è¯•å†…å®¹**ï¼š
- æ¨¡å‹èƒ½å¦æ­£ç¡®åˆ›å»º
- è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
- è¾“å‡ºæ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°å€¼

#### 2. ä¸åŒè¾“å…¥å°ºå¯¸æµ‹è¯• (`test_simple_vit_different_sizes`)
```python
test_configs = [
    {"image_size": 32, "patch_size": 4, "dim": 128, "depth": 3},
    {"image_size": 64, "patch_size": 8, "dim": 256, "depth": 4},
    {"image_size": 128, "patch_size": 16, "dim": 384, "depth": 5},
]
```

**æµ‹è¯•å†…å®¹**ï¼š
- ä¸åŒå›¾åƒå°ºå¯¸çš„é€‚åº”æ€§
- ä¸åŒpatchå°ºå¯¸çš„å¤„ç†
- ä¸åŒæ¨¡å‹è§„æ¨¡çš„ç¨³å®šæ€§

#### 3. æ¢¯åº¦è®¡ç®—æµ‹è¯• (`test_simple_vit_gradients`)
```python
def test_simple_vit_gradients():
    model = SimpleViT(...)
    input_tensor = torch.randn(1, 3, 32, 32, requires_grad=True)
    target = torch.tensor([2])
    
    output = model(input_tensor)
    loss = nn.CrossEntropyLoss()(output, target)
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£ç¡®è®¡ç®—
    for name, param in model.named_parameters():
        if param.grad is not None:
            assert not torch.isnan(param.grad).any()
```

**æµ‹è¯•å†…å®¹**ï¼š
- æ¢¯åº¦æ˜¯å¦æ­£ç¡®è®¡ç®—
- æ˜¯å¦å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
- åå‘ä¼ æ’­çš„æ­£ç¡®æ€§

#### 4. æ¨¡å‹ç»„ä»¶æµ‹è¯• (`test_model_components`)
```python
def test_model_components():
    model = SimpleViT(...)
    
    # æµ‹è¯•patch embedding
    patches = model.to_patch_embedding(input_tensor)
    assert patches.shape == (1, expected_patches, 64)
    
    # æµ‹è¯•ä½ç½®ç¼–ç 
    pos_embed = model.pos_embedding
    assert pos_embed.shape == (expected_patches, 64)
```

**æµ‹è¯•å†…å®¹**ï¼š
- å„ä¸ªç»„ä»¶çš„è¾“å‡ºå½¢çŠ¶
- ç»„ä»¶ä¹‹é—´çš„æ•°æ®æµ
- å†…éƒ¨è®¡ç®—çš„æ­£ç¡®æ€§

### è¿è¡Œæµ‹è¯•

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ
cd pytorch-examples/vit-pytorch
python test/ch01/01-simple_vit.py
```

**æœŸæœ›è¾“å‡º**ï¼š
```
å¼€å§‹ Simple ViT æµ‹è¯•...
=== Simple ViT åŸºæœ¬åŠŸèƒ½æµ‹è¯• ===
æ¨¡å‹å‚æ•°æ€»æ•°: 21,123,456
è¾“å…¥å¼ é‡å½¢çŠ¶: torch.Size([2, 3, 224, 224])
è¾“å‡ºå¼ é‡å½¢çŠ¶: torch.Size([2, 1000])
æœŸæœ›è¾“å‡ºå½¢çŠ¶: (2, 1000)
âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡

=== Simple ViT ä¸åŒè¾“å…¥å°ºå¯¸æµ‹è¯• ===
...
âœ… ä¸åŒå°ºå¯¸æµ‹è¯•å…¨éƒ¨é€šè¿‡

=== Simple ViT æ¢¯åº¦è®¡ç®—æµ‹è¯• ===
...
âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡

=== æ¨¡å‹ç»„ä»¶æµ‹è¯• ===
...
âœ… æ¨¡å‹ç»„ä»¶æµ‹è¯•é€šè¿‡

ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼
```

---

## ğŸš€ æ¨¡å‹è®­ç»ƒ

### è®­ç»ƒç¯å¢ƒå‡†å¤‡

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œç¡®ä¿ä½ çš„ç¯å¢ƒæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

```bash
# å®‰è£…å¿…è¦çš„ä¾èµ–
pip install torch torchvision tqdm matplotlib

# æ£€æŸ¥è®¾å¤‡æ”¯æŒæƒ…å†µ
python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
if hasattr(torch.backends, 'mps'):
    print(f'MPS available: {torch.backends.mps.is_available()}')
print(f'PyTorch version: {torch.__version__}')
"
```

### ğŸ Apple Silicon æ”¯æŒ

æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬å¯¹ Apple M1/M2/M3 èŠ¯ç‰‡è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼š

```python
def get_device():
    """æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆçº§ CUDA > MPS > CPU"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = torch.cuda.get_device_name(0)
        print(f"ğŸš€ ä½¿ç”¨ CUDA è®¾å¤‡: {device_name}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"ğŸ ä½¿ç”¨ Apple Silicon MPS åŠ é€Ÿ")
    else:
        device = torch.device('cpu')
        print(f"ğŸ’» ä½¿ç”¨ CPU è®¾å¤‡")

    return device
```

**MPS ä¼˜åŠ¿**ï¼š
- âš¡ æ¯” CPU å¿« 3-5 å€
- ğŸ’¾ ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œæ›´é«˜æ•ˆçš„å†…å­˜ä½¿ç”¨
- ğŸ”‹ æ›´ä½åŠŸè€—å’Œå‘çƒ­
- ğŸ¯ é’ˆå¯¹ Apple Silicon æ¶æ„ä¼˜åŒ–

### è®­ç»ƒè„šæœ¬æ¦‚è§ˆ

æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬ `train_simple_vit.py` åŒ…å«ä»¥ä¸‹ä¸»è¦åŠŸèƒ½ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼šä½¿ç”¨CIFAR-10æ•°æ®é›†
2. **æ¨¡å‹åˆ›å»º**ï¼šé…ç½®SimpleViTæ¨¡å‹
3. **è®­ç»ƒå¾ªç¯**ï¼šå®Œæ•´çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹
4. **æ¨¡å‹ä¿å­˜**ï¼šä¿å­˜æœ€ä½³æ¨¡å‹å’Œè®­ç»ƒå†å²

### æ•°æ®åŠ è½½å’Œé¢„å¤„ç†

```python
def create_data_loaders(batch_size=32, num_workers=4):
    # æ™ºèƒ½æ•°æ®é›†æ£€æµ‹
    data_dir = './data'
    cifar10_dir = os.path.join(data_dir, 'cifar-10-batches-py')
    cifar10_tar = os.path.join(data_dir, 'cifar-10-python.tar.gz')

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
    if os.path.exists(cifar10_dir):
        print("âœ… æ£€æµ‹åˆ°å·²è§£å‹çš„CIFAR-10æ•°æ®é›†ï¼Œè·³è¿‡ä¸‹è½½")
        download_flag = False
    elif os.path.exists(cifar10_tar):
        print("âœ… æ£€æµ‹åˆ°CIFAR-10å‹ç¼©åŒ…ï¼Œè·³è¿‡ä¸‹è½½ï¼Œå°†è‡ªåŠ¨è§£å‹")
        download_flag = False
    else:
        print("ğŸ“¥ CIFAR-10æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        download_flag = True

    # è®­ç»ƒæ•°æ®å¢å¼º
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),  # è°ƒæ•´åˆ°ViTæœŸæœ›å°ºå¯¸
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # æµ‹è¯•æ•°æ®é¢„å¤„ç†
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½CIFAR-10æ•°æ®é›†ï¼ˆæ™ºèƒ½ä¸‹è½½ï¼‰
    train_dataset = torchvision.datasets.CIFAR10(
        root=data_dir, train=True, download=download_flag, transform=transform_train
    )
    test_dataset = torchvision.datasets.CIFAR10(
        root=data_dir, train=False, download=download_flag, transform=transform_test
    )

    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True), \
           DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```

**å…³é”®ç‚¹**ï¼š
- **æ™ºèƒ½ä¸‹è½½**ï¼šè‡ªåŠ¨æ£€æµ‹å·²å­˜åœ¨çš„æ•°æ®é›†ï¼Œé¿å…é‡å¤ä¸‹è½½
- **æ•°æ®å¢å¼º**ï¼šéšæœºç¿»è½¬å’Œæ—‹è½¬æé«˜æ³›åŒ–èƒ½åŠ›
- **æ ‡å‡†åŒ–**ï¼šä½¿ç”¨ImageNetçš„å‡å€¼å’Œæ ‡å‡†å·®
- **å°ºå¯¸è°ƒæ•´**ï¼šCIFAR-10åŸå§‹32x32è°ƒæ•´åˆ°224x224

### æ¨¡å‹é…ç½®

```python
def create_model():
    model = SimpleViT(
        image_size=224,      # è¾“å…¥å›¾åƒå°ºå¯¸
        patch_size=16,       # patchå°ºå¯¸ (224/16 = 14x14 patches)
        num_classes=10,      # CIFAR-10æœ‰10ä¸ªç±»åˆ«
        dim=512,             # æ¨¡å‹ç»´åº¦
        depth=6,             # Transformerå±‚æ•°
        heads=8,             # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
        mlp_dim=1024,        # MLPéšè—å±‚ç»´åº¦
        channels=3,          # RGBå›¾åƒ
        dim_head=64          # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
    )
    return model
```

**å‚æ•°é€‰æ‹©è¯´æ˜**ï¼š
- `patch_size=16`ï¼šå¹³è¡¡è®¡ç®—æ•ˆç‡å’Œç‰¹å¾ç²¾åº¦
- `dim=512`ï¼šé€‚ä¸­çš„æ¨¡å‹å®¹é‡ï¼Œé¿å…è¿‡æ‹Ÿåˆ
- `depth=6`ï¼šè¶³å¤Ÿçš„å±‚æ•°æå–å¤æ‚ç‰¹å¾
- `heads=8`ï¼šå¤šå¤´æ³¨æ„åŠ›å¢å¼ºè¡¨è¾¾èƒ½åŠ›

### è®­ç»ƒå¾ªç¯å®ç°

```python
def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')

    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡å‡†ç¡®ç‡
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.3f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })

    return running_loss / len(train_loader), 100. * correct / total
```

### è¿è¡Œè®­ç»ƒ

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd pytorch-examples/vit-pytorch

# è¿è¡Œè®­ç»ƒè„šæœ¬
python test/ch01/train_simple_vit.py
```

**è®­ç»ƒè¾“å‡ºç¤ºä¾‹**ï¼š
```
SimpleViT è®­ç»ƒç¤ºä¾‹
==================================================
ä½¿ç”¨è®¾å¤‡: cuda
=== å‡†å¤‡æ•°æ®é›† ===
è®­ç»ƒé›†å¤§å°: 50000
æµ‹è¯•é›†å¤§å°: 10000
æ‰¹æ¬¡å¤§å°: 32
=== åˆ›å»ºæ¨¡å‹ ===
æ¨¡å‹æ€»å‚æ•°æ•°: 21,673,994
å¯è®­ç»ƒå‚æ•°æ•°: 21,673,994

=== å¼€å§‹è®­ç»ƒ ===
è®­ç»ƒè½®æ•°: 10
å­¦ä¹ ç‡: 0.0003
æƒé‡è¡°å‡: 0.0001

Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:15<00:00, Loss: 2.156, Acc: 18.23%]
Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:15<00:00]

Epoch 1/10:
  è®­ç»ƒæŸå¤±: 2.1564, è®­ç»ƒå‡†ç¡®ç‡: 18.23%
  æµ‹è¯•æŸå¤±: 2.0234, æµ‹è¯•å‡†ç¡®ç‡: 25.67%
  å­¦ä¹ ç‡: 0.000300
  è€—æ—¶: 150.23s
  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: 25.67%)
```

### è®­ç»ƒæŠ€å·§å’Œä¼˜åŒ–

#### 1. å­¦ä¹ ç‡è°ƒåº¦
```python
# ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
```

#### 2. æƒé‡è¡°å‡
```python
# AdamWä¼˜åŒ–å™¨withæƒé‡è¡°å‡
optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
```

#### 3. æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

#### 4. æ—©åœç­–ç•¥
```python
# å¦‚æœéªŒè¯å‡†ç¡®ç‡ä¸å†æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ
if test_acc > best_acc:
    best_acc = test_acc
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        print("Early stopping triggered")
        break
```

---

## ğŸ” æ¨¡å‹æ¨ç†

### æ¨ç†è„šæœ¬åŠŸèƒ½

`inference_simple_vit.py` æä¾›äº†å¤šç§æ¨ç†æ¨¡å¼ï¼š

1. **å•å¼ å›¾åƒé¢„æµ‹**ï¼šå¯¹å•ä¸ªå›¾åƒè¿›è¡Œåˆ†ç±»
2. **æ‰¹é‡å›¾åƒé¢„æµ‹**ï¼šå¤„ç†æ•´ä¸ªæ–‡ä»¶å¤¹çš„å›¾åƒ
3. **éšæœºæ•°æ®æ¼”ç¤º**ï¼šä½¿ç”¨éšæœºæ•°æ®æµ‹è¯•æ¨¡å‹

### æ¨¡å‹åŠ è½½

```python
def load_model(model_path, device):
    # åˆ›å»ºæ¨¡å‹æ¶æ„
    model = SimpleViT(
        image_size=224, patch_size=16, num_classes=10,
        dim=512, depth=6, heads=8, mlp_dim=1024
    )

    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    return model
```

### å›¾åƒé¢„å¤„ç†

```python
def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦

    return input_tensor, image
```

### é¢„æµ‹å‡½æ•°

```python
def predict_single_image(model, image_path, device):
    # é¢„å¤„ç†å›¾åƒ
    input_tensor, original_image = preprocess_image(image_path)
    input_tensor = input_tensor.to(device)

    # æ¨¡å‹é¢„æµ‹
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = F.softmax(outputs, dim=1)
        confidence, predicted = torch.max(probabilities, 1)

    # è·å–é¢„æµ‹ç»“æœ
    predicted_class = CIFAR10_CLASSES[predicted.item()]
    confidence_score = confidence.item()

    # æ˜¾ç¤ºTop-5é¢„æµ‹
    top5_prob, top5_indices = torch.topk(probabilities, 5)
    for i in range(5):
        class_name = CIFAR10_CLASSES[top5_indices[0][i]]
        prob = top5_prob[0][i].item()
        print(f"  {i+1}. {class_name}: {prob:.4f}")

    return predicted_class, confidence_score
```

### è¿è¡Œæ¨ç†

```bash
# è¿è¡Œæ¨ç†è„šæœ¬
python test/ch01/inference_simple_vit.py

# é€‰æ‹©æ¨ç†æ¨¡å¼
=== é€‰æ‹©æ¼”ç¤ºæ¨¡å¼ ===
1. å•å¼ å›¾åƒé¢„æµ‹
2. æ‰¹é‡å›¾åƒé¢„æµ‹
3. éšæœºæ•°æ®æ¼”ç¤º
è¯·é€‰æ‹©æ¨¡å¼ (1/2/3): 1

# è¾“å…¥å›¾åƒè·¯å¾„
è¯·è¾“å…¥å›¾åƒè·¯å¾„: /path/to/your/image.jpg
```

**æ¨ç†è¾“å‡ºç¤ºä¾‹**ï¼š
```
=== é¢„æµ‹å›¾åƒ: cat.jpg ===
é¢„æµ‹ç±»åˆ«: cat
ç½®ä¿¡åº¦: 0.8234

å‰5ä¸ªé¢„æµ‹ç»“æœ:
  1. cat: 0.8234
  2. dog: 0.1123
  3. horse: 0.0234
  4. deer: 0.0198
  5. bird: 0.0156
```

### æ‰¹é‡æ¨ç†

```python
def predict_batch_images(model, image_folder, device, max_images=10):
    # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
    image_files = []

    for file in os.listdir(image_folder):
        if any(file.lower().endswith(ext) for ext in image_extensions):
            image_files.append(os.path.join(image_folder, file))

    # æ‰¹é‡é¢„æµ‹
    results = []
    for image_path in image_files[:max_images]:
        predicted_class, confidence = predict_single_image(
            model, image_path, device
        )
        results.append({
            'image': os.path.basename(image_path),
            'prediction': predicted_class,
            'confidence': confidence
        })

    return results
```

---

## ğŸ’¡ å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šä¿®æ”¹æ¨¡å‹å‚æ•°
å°è¯•ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼Œè§‚å¯Ÿå¯¹è®­ç»ƒæ•ˆæœçš„å½±å“ï¼š

```python
# åŸå§‹é…ç½®
model = SimpleViT(
    image_size=224,
    patch_size=16,      # å°è¯•æ”¹ä¸º32, 8
    num_classes=10,     # CIFAR-10
    dim=512,           # å°è¯•æ”¹ä¸º256, 1024
    depth=6,           # å°è¯•æ”¹ä¸º3, 12
    heads=8,           # å°è¯•æ”¹ä¸º4, 16
    mlp_dim=1024       # å°è¯•æ”¹ä¸º512, 2048
)
```

**å®éªŒä»»åŠ¡**ï¼š
1. æ¯”è¾ƒä¸åŒpatch_sizeå¯¹è®­ç»ƒé€Ÿåº¦å’Œå‡†ç¡®ç‡çš„å½±å“
2. æµ‹è¯•æ¨¡å‹æ·±åº¦å¯¹æ”¶æ•›é€Ÿåº¦çš„å½±å“
3. åˆ†ææ³¨æ„åŠ›å¤´æ•°é‡ä¸æ¨¡å‹æ€§èƒ½çš„å…³ç³»

**è®°å½•è¡¨æ ¼**ï¼š
| patch_size | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´/epoch | æœ€ç»ˆå‡†ç¡®ç‡ |
|------------|--------|----------------|------------|
| 8          |        |                |            |
| 16         |        |                |            |
| 32         |        |                |            |

### ç»ƒä¹ 2ï¼šåˆ†ææ¨¡å‹å¤æ‚åº¦
```python
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def analyze_model_complexity(image_size, patch_size, dim, depth, heads):
    model = SimpleViT(
        image_size=image_size,
        patch_size=patch_size,
        num_classes=1000,
        dim=dim,
        depth=depth,
        heads=heads,
        mlp_dim=dim*4
    )
    
    params = count_parameters(model)
    print(f"å‚æ•°æ•°é‡: {params:,}")
    
    # è®¡ç®—FLOPs
    num_patches = (image_size // patch_size) ** 2
    attention_flops = num_patches * num_patches * dim * depth * heads
    print(f"æ³¨æ„åŠ›è®¡ç®—FLOPs: {attention_flops:,}")
```

### ç»ƒä¹ 3ï¼šè®­ç»ƒç­–ç•¥ä¼˜åŒ–
å°è¯•ä¸åŒçš„è®­ç»ƒç­–ç•¥ï¼Œè§‚å¯Ÿå¯¹æ€§èƒ½çš„å½±å“ï¼š

```python
# å®éªŒ1ï¼šä¸åŒçš„ä¼˜åŒ–å™¨
optimizers = {
    'Adam': optim.Adam(model.parameters(), lr=3e-4),
    'AdamW': optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4),
    'SGD': optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
}

# å®éªŒ2ï¼šä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦
schedulers = {
    'CosineAnnealing': optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs),
    'StepLR': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5),
    'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)
}

# å®éªŒ3ï¼šæ•°æ®å¢å¼ºç­–ç•¥
augmentations = {
    'basic': [transforms.RandomHorizontalFlip()],
    'medium': [transforms.RandomHorizontalFlip(), transforms.RandomRotation(10)],
    'strong': [transforms.RandomHorizontalFlip(), transforms.RandomRotation(15),
               transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)]
}
```

### ç»ƒä¹ 4ï¼šæ¨¡å‹æ¨ç†ä¼˜åŒ–
å®ç°æ¨ç†æ€§èƒ½ä¼˜åŒ–ï¼š

```python
# 1. æ¨¡å‹é‡åŒ–
def quantize_model(model):
    model.eval()
    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )
    return quantized_model

# 2. æ‰¹é‡æ¨ç†
def batch_inference(model, image_list, batch_size=32):
    model.eval()
    results = []

    for i in range(0, len(image_list), batch_size):
        batch = image_list[i:i+batch_size]
        batch_tensor = torch.stack([preprocess_image(img)[0] for img in batch])

        with torch.no_grad():
            outputs = model(batch_tensor)
            predictions = F.softmax(outputs, dim=1)

        results.extend(predictions.cpu().numpy())

    return results

# 3. æ¨ç†æ—¶é—´æµ‹è¯•
def benchmark_inference(model, input_size=(1, 3, 224, 224), num_runs=100):
    model.eval()
    dummy_input = torch.randn(input_size)

    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)

    # è®¡æ—¶
    import time
    start_time = time.time()
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model(dummy_input)
    end_time = time.time()

    avg_time = (end_time - start_time) / num_runs
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}ms")
```

### ç»ƒä¹ 5ï¼šå¯è§†åŒ–åˆ†æ
å®ç°è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡å‹è¡Œä¸ºçš„å¯è§†åŒ–ï¼š

```python
# 1. è®­ç»ƒæ›²çº¿å¯è§†åŒ–
def plot_training_curves(train_losses, train_accs, test_losses, test_accs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(test_losses, label='Test Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.set_title('Training and Test Loss')

    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accs, label='Train Acc')
    ax2.plot(test_accs, label='Test Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.set_title('Training and Test Accuracy')

    plt.tight_layout()
    plt.show()

# 2. æ··æ·†çŸ©é˜µ
def plot_confusion_matrix(model, test_loader, class_names):
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            pred = output.argmax(dim=1)
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

    cm = confusion_matrix(all_targets, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

# 3. ç‰¹å¾å¯è§†åŒ–
def visualize_patch_embeddings(model, image):
    """å¯è§†åŒ–patch embeddings"""
    model.eval()

    with torch.no_grad():
        # è·å–patch embeddings
        patches = model.to_patch_embedding(image.unsqueeze(0))

        # ä½¿ç”¨PCAé™ç»´åˆ°2D
        from sklearn.decomposition import PCA
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(patches[0].cpu().numpy())

        # å¯è§†åŒ–
        plt.figure(figsize=(8, 6))
        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
        plt.title('Patch Embeddings Visualization (PCA)')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.show()
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆè¦ä½¿ç”¨patch embeddingï¼Ÿ
**A**: 
- å°†å›¾åƒè½¬æ¢ä¸ºåºåˆ—ï¼Œä½¿å¾—Transformerèƒ½å¤Ÿå¤„ç†
- æ¯ä¸ªpatchåŒ…å«å±€éƒ¨çš„ç©ºé—´ä¿¡æ¯
- çº¿æ€§æŠ•å½±å­¦ä¹ æ›´å¥½çš„ç‰¹å¾è¡¨ç¤º

### Q2: ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
**A**:
- Transformeræœ¬èº«æ²¡æœ‰ä½ç½®ä¿¡æ¯
- ä½ç½®ç¼–ç è®©æ¨¡å‹ç†è§£patchä¹‹é—´çš„ç©ºé—´å…³ç³»
- 2Dä½ç½®ç¼–ç æ¯”1Dæ›´é€‚åˆå›¾åƒæ•°æ®

### Q3: ä¸ºä»€ä¹ˆä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ
**A**:
- ä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„ç‰¹å¾
- å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›
- ç±»ä¼¼äºCNNä¸­çš„å¤šä¸ªå·ç§¯æ ¸

### Q4: SimpleViTä¸æ ‡å‡†ViTçš„ä¸»è¦åŒºåˆ«ï¼Ÿ
**A**:
- æ²¡æœ‰CLS tokenï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
- å›ºå®šçš„ä½ç½®ç¼–ç ï¼Œä¸éœ€è¦å­¦ä¹ 
- æ²¡æœ‰dropoutï¼Œç»“æ„æ›´ç®€å•

### Q5: å¦‚ä½•é€‰æ‹©åˆé€‚çš„patch sizeï¼Ÿ
**A**:
- è¾ƒå°çš„patch sizeï¼šæ›´ç²¾ç»†çš„ç‰¹å¾ï¼Œä½†è®¡ç®—é‡å¤§
- è¾ƒå¤§çš„patch sizeï¼šè®¡ç®—æ•ˆç‡é«˜ï¼Œä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚
- é€šå¸¸é€‰æ‹©16x16æˆ–32x32

---

## ğŸ“ˆ è¿›é˜¶å­¦ä¹ 

### 1. æ¨¡å‹æ”¹è¿›æ–¹å‘
- **æ•ˆç‡ä¼˜åŒ–**ï¼šçº¿æ€§æ³¨æ„åŠ›ã€ç¨€ç–æ³¨æ„åŠ›
- **æ¶æ„åˆ›æ–°**ï¼šåˆ†å±‚ViTã€æ··åˆCNN-ViT
- **é¢„è®­ç»ƒç­–ç•¥**ï¼šMAEã€DINOç­‰è‡ªç›‘ç£æ–¹æ³•

### 2. ç›¸å…³æŠ€æœ¯
- **Swin Transformer**ï¼šçª—å£æ³¨æ„åŠ›æœºåˆ¶
- **DeiT**ï¼šæ•°æ®é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥
- **CaiT**ï¼šæ·±åº¦ViTçš„è®­ç»ƒæŠ€å·§

### 3. å®é™…åº”ç”¨
- **å›¾åƒåˆ†ç±»**ï¼šImageNetåˆ†ç±»ä»»åŠ¡
- **ç›®æ ‡æ£€æµ‹**ï¼šDETRç³»åˆ—æ¨¡å‹
- **å›¾åƒåˆ†å‰²**ï¼šSegFormerç­‰

### 4. è¿›ä¸€æ­¥å­¦ä¹ èµ„æº
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
- [DeiT: Data-efficient Image Transformers](https://arxiv.org/abs/2012.12877)

---

## ğŸ¯ æ€»ç»“

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„æ•™ç¨‹ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†ï¼š

1. **ç†è®ºåŸºç¡€**ï¼šVision Transformerçš„æ ¸å¿ƒæ¦‚å¿µå’ŒSimpleViTçš„è®¾è®¡æ€æƒ³
2. **ä»£ç å®ç°**ï¼šæ¯ä¸ªç»„ä»¶çš„è¯¦ç»†å®ç°å’Œæ¶æ„è§£æ
3. **æµ‹è¯•éªŒè¯**ï¼šå¦‚ä½•ç¡®ä¿æ¨¡å‹çš„æ­£ç¡®æ€§å’Œç¨³å®šæ€§
4. **æ¨¡å‹è®­ç»ƒ**ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹ä¿å­˜
5. **æ¨¡å‹æ¨ç†**ï¼šå¤šç§æ¨ç†æ¨¡å¼å’Œæ€§èƒ½ä¼˜åŒ–æŠ€å·§
6. **å®è·µåº”ç”¨**ï¼šçœŸå®åœºæ™¯ä¸­çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ

### ğŸ† å­¦ä¹ æˆæœ

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… ç†è§£Vision Transformerçš„å·¥ä½œåŸç†
- âœ… å®ç°å’Œä¿®æ”¹SimpleViTæ¨¡å‹
- âœ… ç‹¬ç«‹å®Œæˆæ¨¡å‹è®­ç»ƒå’Œè°ƒä¼˜
- âœ… éƒ¨ç½²æ¨¡å‹è¿›è¡Œå®é™…æ¨ç†
- âœ… åˆ†æå’Œå¯è§†åŒ–æ¨¡å‹æ€§èƒ½
- âœ… è§£å†³å¸¸è§çš„è®­ç»ƒå’Œæ¨ç†é—®é¢˜

### ğŸ“Š é¡¹ç›®æ–‡ä»¶æ€»è§ˆ

```
test/ch01/
â”œâ”€â”€ 01-simple_vit.py          # âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•
â”œâ”€â”€ train_simple_vit.py       # âœ… å®Œæ•´è®­ç»ƒè„šæœ¬
â””â”€â”€ inference_simple_vit.py   # âœ… æ¨ç†å’Œéƒ¨ç½²è„šæœ¬
```

### ğŸš€ è¿›é˜¶è·¯å¾„

**åˆçº§é˜¶æ®µ**ï¼ˆå·²å®Œæˆï¼‰ï¼š
- [x] ç†è§£SimpleViTåŸºæœ¬æ¦‚å¿µ
- [x] å®ŒæˆåŸºç¡€è®­ç»ƒå’Œæ¨ç†
- [x] æŒæ¡æ¨¡å‹è°ƒè¯•æŠ€å·§

**ä¸­çº§é˜¶æ®µ**ï¼ˆå»ºè®®ä¸‹ä¸€æ­¥ï¼‰ï¼š
- [ ] åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆå¦‚ImageNetï¼‰
- [ ] å­¦ä¹ æ ‡å‡†ViTå’Œå…¶ä»–å˜ç§
- [ ] å®ç°æ¨¡å‹è’¸é¦å’Œå‹ç¼©
- [ ] æ¢ç´¢å¤šæ¨¡æ€åº”ç”¨

**é«˜çº§é˜¶æ®µ**ï¼ˆé•¿æœŸç›®æ ‡ï¼‰ï¼š
- [ ] ç ”ç©¶æœ€æ–°çš„Transformeræ¶æ„
- [ ] å¼€å‘è‡ªå·±çš„ViTå˜ç§
- [ ] å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®
- [ ] å‘è¡¨ç›¸å…³ç ”ç©¶è®ºæ–‡

### ğŸ’¡ å®ç”¨æŠ€å·§æ€»ç»“

1. **è®­ç»ƒæŠ€å·§**ï¼š
   - ä½¿ç”¨é€‚å½“çš„æ•°æ®å¢å¼º
   - é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡è°ƒåº¦
   - ç›‘æ§è®­ç»ƒæ›²çº¿ï¼ŒåŠæ—¶è°ƒæ•´

2. **æ¨ç†ä¼˜åŒ–**ï¼š
   - æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
   - æ¨¡å‹é‡åŒ–å‡å°‘å†…å­˜å ç”¨
   - ç¼“å­˜é¢„å¤„ç†ç»“æœ

3. **è°ƒè¯•æ–¹æ³•**ï¼š
   - ä»å°æ¨¡å‹å¼€å§‹å®éªŒ
   - å¯è§†åŒ–ä¸­é—´ç»“æœ
   - å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ

### ğŸŒŸ æœ€åçš„è¯

SimpleViTä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç»ä½³çš„å­¦ä¹ å¹³å°ï¼Œå¸®åŠ©ç†è§£Vision Transformerçš„æ ¸å¿ƒæ€æƒ³ã€‚é€šè¿‡åŠ¨æ‰‹å®è·µè®­ç»ƒå’Œæ¨ç†ï¼Œä½ å·²ç»æŒæ¡äº†æ·±åº¦å­¦ä¹ é¡¹ç›®çš„å®Œæ•´æµç¨‹ã€‚

è®°ä½ï¼Œæ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä¿æŒå­¦ä¹ å’Œå®è·µçš„çƒ­æƒ…ï¼Œå…³æ³¨æœ€æ–°çš„ç ”ç©¶è¿›å±•ï¼Œä½ ä¸€å®šèƒ½åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—æ›´å¤§çš„æˆå°±ï¼

**ç»§ç»­å­¦ä¹ èµ„æº**ï¼š
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Papers With Code - Vision Transformer](https://paperswithcode.com/method/vision-transformer)
- [PyTorchå®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)

ç¥ä½ åœ¨Vision Transformerå’Œæ·±åº¦å­¦ä¹ çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼ğŸš€âœ¨