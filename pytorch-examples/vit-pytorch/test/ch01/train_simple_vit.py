#!/usr/bin/env python3
"""
SimpleViT è®­ç»ƒç¤ºä¾‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ SimpleViT è¿›è¡Œå›¾åƒåˆ†ç±»è®­ç»ƒï¼ŒåŒ…å«å†…å­˜ä¼˜åŒ–æŠ€æœ¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import sys
import os
import time
import gc
import psutil
from tqdm import tqdm

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥vit_pytorchæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

from vit_pytorch import SimpleViT

def get_memory_info():
    """è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / 1024 / 1024  # MB

def clear_memory(device):
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    elif device.type == 'mps':
        torch.mps.empty_cache()

def create_data_loaders(batch_size=16, num_workers=2):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    ä½¿ç”¨ CIFAR-10 æ•°æ®é›†ä½œä¸ºç¤ºä¾‹
    """
    print("=== å‡†å¤‡æ•°æ®é›† ===")

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
    data_dir = './data'
    cifar10_dir = os.path.join(data_dir, 'cifar-10-batches-py')
    cifar10_tar = os.path.join(data_dir, 'cifar-10-python.tar.gz')

    # æ£€æŸ¥è§£å‹åçš„æ•°æ®æˆ–å‹ç¼©åŒ…æ˜¯å¦å­˜åœ¨
    if os.path.exists(cifar10_dir):
        print("âœ… æ£€æµ‹åˆ°å·²è§£å‹çš„CIFAR-10æ•°æ®é›†ï¼Œè·³è¿‡ä¸‹è½½")
        download_flag = False
    elif os.path.exists(cifar10_tar):
        print("âœ… æ£€æµ‹åˆ°CIFAR-10å‹ç¼©åŒ…ï¼Œè·³è¿‡ä¸‹è½½ï¼Œå°†è‡ªåŠ¨è§£å‹")
        download_flag = False
    else:
        print("ğŸ“¥ CIFAR-10æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        download_flag = True

    # æ•°æ®é¢„å¤„ç† - ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸ä»¥èŠ‚çœå†…å­˜
    transform_train = transforms.Compose([
        transforms.Resize((128, 128)),  # å‡å°åˆ°128x128ä»¥èŠ‚çœå†…å­˜
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    transform_test = transforms.Compose([
        transforms.Resize((128, 128)),  # å‡å°åˆ°128x128ä»¥èŠ‚çœå†…å­˜
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®é›†ï¼ˆåªåœ¨éœ€è¦æ—¶ä¸‹è½½ï¼‰
    train_dataset = torchvision.datasets.CIFAR10(
        root=data_dir, train=True, download=download_flag, transform=transform_train
    )

    test_dataset = torchvision.datasets.CIFAR10(
        root=data_dir, train=False, download=download_flag, transform=transform_test
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å†…å­˜ä¼˜åŒ–é…ç½®
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=False  # ç¦ç”¨pin_memoryä»¥èŠ‚çœå†…å­˜
    )

    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=False  # ç¦ç”¨pin_memoryä»¥èŠ‚çœå†…å­˜
    )

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å›¾åƒå°ºå¯¸: 128x128 (å†…å­˜ä¼˜åŒ–)")

    return train_loader, test_loader

def create_model():
    """
    åˆ›å»º SimpleViT æ¨¡å‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    """
    print("=== åˆ›å»ºæ¨¡å‹ ===")

    model = SimpleViT(
        image_size=128,      # å‡å°è¾“å…¥å›¾åƒå°ºå¯¸ä»¥èŠ‚çœå†…å­˜
        patch_size=16,       # patch å°ºå¯¸
        num_classes=10,      # CIFAR-10 æœ‰ 10 ä¸ªç±»åˆ«
        dim=384,             # å‡å°æ¨¡å‹ç»´åº¦ä»¥èŠ‚çœå†…å­˜
        depth=4,             # å‡å°‘Transformerå±‚æ•°
        heads=6,             # å‡å°‘å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
        mlp_dim=768,         # å‡å°MLPéšè—å±‚ç»´åº¦
        channels=3,          # RGB å›¾åƒ
        dim_head=64          # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
    )

    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"æ¨¡å‹æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    print("ğŸ“Š å†…å­˜ä¼˜åŒ–é…ç½®:")
    print(f"  - å›¾åƒå°ºå¯¸: 128x128 (åŸ224x224)")
    print(f"  - æ¨¡å‹ç»´åº¦: 384 (åŸ512)")
    print(f"  - å±‚æ•°: 4 (åŸ6)")
    print(f"  - æ³¨æ„åŠ›å¤´æ•°: 6 (åŸ8)")

    return model

def train_epoch(model, train_loader, criterion, optimizer, device, epoch, accumulation_steps=2):
    """
    è®­ç»ƒä¸€ä¸ª epoch - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')

    # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
    accumulation_count = 0

    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)

        # æ¢¯åº¦ç´¯ç§¯ï¼šå°†æŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
        loss = loss / accumulation_steps

        # åå‘ä¼ æ’­
        loss.backward()

        accumulation_count += 1

        # å½“è¾¾åˆ°ç´¯ç§¯æ­¥æ•°æ—¶ï¼Œæ›´æ–°å‚æ•°
        if accumulation_count % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

        # ç»Ÿè®¡
        running_loss += loss.item() * accumulation_steps  # æ¢å¤åŸå§‹æŸå¤±å€¼
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

        # å®šæœŸæ¸…ç†å†…å­˜
        if batch_idx % 100 == 0:
            clear_memory(device)

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.3f}',
            'Acc': f'{100.*correct/total:.2f}%',
            'Mem': f'{get_memory_info():.0f}MB'
        })

    # å¤„ç†å‰©ä½™çš„æ¢¯åº¦
    if accumulation_count % accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

def validate(model, test_loader, criterion, device):
    """
    éªŒè¯æ¨¡å‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    """
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        pbar = tqdm(test_loader, desc='Validating')
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()

            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 50 == 0:
                clear_memory(device)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Acc': f'{100.*correct/total:.2f}%',
                'Mem': f'{get_memory_info():.0f}MB'
            })

    test_loss /= len(test_loader)
    test_acc = 100. * correct / total

    return test_loss, test_acc

def get_device():
    """
    æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆçº§ CUDA > MPS > CPU
    """
    if torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = torch.cuda.get_device_name(0)
        print(f"ğŸš€ ä½¿ç”¨ CUDA è®¾å¤‡: {device_name}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"ğŸ ä½¿ç”¨ Apple Silicon MPS åŠ é€Ÿ")
    else:
        device = torch.device('cpu')
        print(f"ğŸ’» ä½¿ç”¨ CPU è®¾å¤‡")

    return device

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•° - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
    """
    print("SimpleViT è®­ç»ƒç¤ºä¾‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 60)

    # æ™ºèƒ½è®¾å¤‡é€‰æ‹©
    device = get_device()

    # è®­ç»ƒå‚æ•° - å†…å­˜ä¼˜åŒ–é…ç½®
    batch_size = 16          # å‡å°æ‰¹æ¬¡å¤§å°
    accumulation_steps = 2   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæœ‰æ•ˆæ‰¹æ¬¡å¤§å° = 16 * 2 = 32
    num_epochs = 8           # å‡å°‘è®­ç»ƒè½®æ•°
    learning_rate = 2e-4     # ç¨å¾®é™ä½å­¦ä¹ ç‡
    weight_decay = 1e-4
    patience = 3             # æ—©åœè€å¿ƒå€¼

    print(f"ğŸ’¾ å†…å­˜ä¼˜åŒ–é…ç½®:")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
    print(f"  - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {batch_size * accumulation_steps}")
    print(f"  - åˆå§‹å†…å­˜ä½¿ç”¨: {get_memory_info():.0f}MB")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, test_loader = create_data_loaders(batch_size)

    # åˆ›å»ºæ¨¡å‹
    model = create_model()
    model = model.to(device)

    # æ¸…ç†åˆå§‹å†…å­˜
    clear_memory(device)
    print(f"  - æ¨¡å‹åŠ è½½åå†…å­˜: {get_memory_info():.0f}MB")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    print(f"\n=== å¼€å§‹è®­ç»ƒ ===")
    print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"æƒé‡è¡°å‡: {weight_decay}")
    print(f"æ—©åœè€å¿ƒå€¼: {patience}")

    # è®­ç»ƒå¾ªç¯
    best_acc = 0
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    patience_counter = 0

    for epoch in range(1, num_epochs + 1):
        start_time = time.time()

        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch, accumulation_steps
        )

        # éªŒè¯
        test_loss, test_acc = validate(model, test_loader, criterion, device)

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # è®°å½•ç»“æœ
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)

        epoch_time = time.time() - start_time
        current_memory = get_memory_info()

        print(f'\nEpoch {epoch}/{num_epochs}:')
        print(f'  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%')
        print(f'  æµ‹è¯•æŸå¤±: {test_loss:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%')
        print(f'  å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}')
        print(f'  è€—æ—¶: {epoch_time:.2f}s, å†…å­˜: {current_memory:.0f}MB')

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
            }, 'best_simple_vit_optimized.pth')
            print(f'  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_acc:.2f}%)')
        else:
            patience_counter += 1
            print(f'  â³ æ—©åœè®¡æ•°å™¨: {patience_counter}/{patience}')

        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­{patience}ä¸ªepochæ— æ”¹å–„")
            break

        # å®šæœŸæ¸…ç†å†…å­˜
        clear_memory(device)

    print(f"\n=== è®­ç»ƒå®Œæˆ ===")
    print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"å®é™…è®­ç»ƒè½®æ•°: {epoch}")
    print(f"æœ€ç»ˆå†…å­˜ä½¿ç”¨: {get_memory_info():.0f}MB")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_losses': train_losses,
        'train_accs': train_accs,
        'test_losses': test_losses,
        'test_accs': test_accs,
        'best_acc': best_acc,
    }, 'final_simple_vit_optimized.pth')

    print("\nğŸ“ æ¨¡å‹å·²ä¿å­˜:")
    print("  - best_simple_vit_optimized.pth (æœ€ä½³æ¨¡å‹)")
    print("  - final_simple_vit_optimized.pth (æœ€ç»ˆæ¨¡å‹)")

    # æœ€ç»ˆå†…å­˜æ¸…ç†
    clear_memory(device)

if __name__ == "__main__":
    main()
