#!/usr/bin/env python3
"""
è®¾å¤‡æ£€æµ‹è„šæœ¬
æ£€æŸ¥å½“å‰ç³»ç»Ÿçš„ PyTorch è®¾å¤‡æ”¯æŒæƒ…å†µ
"""

import torch
import platform
import sys

def check_pytorch_info():
    """æ£€æŸ¥ PyTorch åŸºæœ¬ä¿¡æ¯"""
    print("ğŸ” PyTorch ç¯å¢ƒä¿¡æ¯")
    print("=" * 40)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    
    if platform.system() == "Darwin":
        # macOS ç³»ç»Ÿï¼Œæ˜¾ç¤ºèŠ¯ç‰‡ä¿¡æ¯
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                cpu_info = result.stdout.strip()
                print(f"å¤„ç†å™¨: {cpu_info}")
        except:
            pass
    
    print()

def check_cuda_support():
    """æ£€æŸ¥ CUDA æ”¯æŒ"""
    print("ğŸš€ CUDA æ”¯æŒæ£€æŸ¥")
    print("=" * 40)
    
    if torch.cuda.is_available():
        print("âœ… CUDA å¯ç”¨")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # æµ‹è¯• CUDA å¼ é‡æ“ä½œ
        try:
            x = torch.randn(100, 100).cuda()
            y = torch.randn(100, 100).cuda()
            z = torch.mm(x, y)
            print("âœ… CUDA å¼ é‡æ“ä½œæµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âŒ CUDA å¼ é‡æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
    else:
        print("âŒ CUDA ä¸å¯ç”¨")
        if platform.system() == "Windows":
            print("ğŸ’¡ Windows ç”¨æˆ·å¯ä»¥å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch:")
            print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
        elif platform.system() == "Linux":
            print("ğŸ’¡ Linux ç”¨æˆ·å¯ä»¥å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch:")
            print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    
    print()

def check_mps_support():
    """æ£€æŸ¥ Apple MPS æ”¯æŒ"""
    print("ğŸ Apple MPS æ”¯æŒæ£€æŸ¥")
    print("=" * 40)
    
    if platform.system() != "Darwin":
        print("âŒ MPS ä»…åœ¨ macOS ç³»ç»Ÿä¸Šå¯ç”¨")
        print()
        return
    
    if hasattr(torch.backends, 'mps'):
        if torch.backends.mps.is_available():
            print("âœ… MPS å¯ç”¨")
            print("âœ… Apple Silicon åŠ é€Ÿå·²å¯ç”¨")
            
            # æµ‹è¯• MPS å¼ é‡æ“ä½œ
            try:
                x = torch.randn(100, 100).to('mps')
                y = torch.randn(100, 100).to('mps')
                z = torch.mm(x, y)
                print("âœ… MPS å¼ é‡æ“ä½œæµ‹è¯•é€šè¿‡")
                
                # æ€§èƒ½ç®€å•æµ‹è¯•
                import time
                
                # CPU æµ‹è¯•
                x_cpu = torch.randn(1000, 1000)
                y_cpu = torch.randn(1000, 1000)
                start_time = time.time()
                for _ in range(10):
                    z_cpu = torch.mm(x_cpu, y_cpu)
                cpu_time = time.time() - start_time
                
                # MPS æµ‹è¯•
                x_mps = torch.randn(1000, 1000).to('mps')
                y_mps = torch.randn(1000, 1000).to('mps')
                start_time = time.time()
                for _ in range(10):
                    z_mps = torch.mm(x_mps, y_mps)
                    torch.mps.synchronize()  # ç¡®ä¿æ“ä½œå®Œæˆ
                mps_time = time.time() - start_time
                
                speedup = cpu_time / mps_time
                print(f"ğŸš€ MPS ç›¸å¯¹ CPU åŠ é€Ÿæ¯”: {speedup:.1f}x")
                
            except Exception as e:
                print(f"âŒ MPS å¼ é‡æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
                print("ğŸ’¡ å°è¯•æ›´æ–° PyTorch: pip install --upgrade torch torchvision")
        else:
            print("âŒ MPS ä¸å¯ç”¨")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
            print("   - ä¸æ˜¯ Apple Silicon (M1/M2/M3) èŠ¯ç‰‡")
            print("   - PyTorch ç‰ˆæœ¬è¿‡æ—§")
            print("   - macOS ç‰ˆæœ¬è¿‡æ—§ (éœ€è¦ macOS 12.3+)")
    else:
        print("âŒ å½“å‰ PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ MPS")
        print("ğŸ’¡ è¯·æ›´æ–°åˆ°æ”¯æŒ MPS çš„ PyTorch ç‰ˆæœ¬:")
        print("   pip install --upgrade torch torchvision")
    
    print()

def check_cpu_info():
    """æ£€æŸ¥ CPU ä¿¡æ¯"""
    print("ğŸ’» CPU ä¿¡æ¯")
    print("=" * 40)
    
    # æµ‹è¯• CPU æ€§èƒ½
    import time
    
    x = torch.randn(1000, 1000)
    y = torch.randn(1000, 1000)
    
    start_time = time.time()
    for _ in range(10):
        z = torch.mm(x, y)
    cpu_time = time.time() - start_time
    
    print(f"CPU çŸ©é˜µä¹˜æ³•æ€§èƒ½: {cpu_time:.3f}s (10æ¬¡ 1000x1000)")
    print(f"CPU çº¿ç¨‹æ•°: {torch.get_num_threads()}")
    print("âœ… CPU å¯ç”¨")
    print()

def get_recommended_device():
    """è·å–æ¨èçš„è®¾å¤‡"""
    print("ğŸ¯ æ¨èè®¾å¤‡é…ç½®")
    print("=" * 40)
    
    if torch.cuda.is_available():
        device = "cuda"
        print("ğŸš€ æ¨èä½¿ç”¨ CUDA GPU è¿›è¡Œè®­ç»ƒ")
        print("   - æœ€å¿«çš„è®­ç»ƒé€Ÿåº¦")
        print("   - æ”¯æŒå¤§æ‰¹æ¬¡è®­ç»ƒ")
        print("   - é€‚åˆé•¿æ—¶é—´è®­ç»ƒ")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"
        print("ğŸ æ¨èä½¿ç”¨ Apple MPS è¿›è¡Œè®­ç»ƒ")
        print("   - æ¯” CPU å¿« 3-5 å€")
        print("   - ä½åŠŸè€—å’Œå‘çƒ­")
        print("   - ç»Ÿä¸€å†…å­˜æ¶æ„")
    else:
        device = "cpu"
        print("ğŸ’» ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ")
        print("   - å…¼å®¹æ€§æœ€å¥½")
        print("   - è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢")
        print("   - é€‚åˆå°æ¨¡å‹å’Œè°ƒè¯•")
    
    print(f"\næ¨èè®¾å¤‡: {device}")
    print()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ PyTorch è®¾å¤‡æ”¯æŒæ£€æµ‹å·¥å…·")
    print("=" * 50)
    print()
    
    # æ£€æŸ¥å„ç§è®¾å¤‡æ”¯æŒ
    check_pytorch_info()
    check_cuda_support()
    check_mps_support()
    check_cpu_info()
    get_recommended_device()
    
    print("ğŸ‰ æ£€æµ‹å®Œæˆï¼")
    print()
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   - è®­ç»ƒå¤§æ¨¡å‹: ä¼˜å…ˆé€‰æ‹© CUDA GPU")
    print("   - Apple è®¾å¤‡: ä½¿ç”¨ MPS åŠ é€Ÿ")
    print("   - è°ƒè¯•å’Œå°å®éªŒ: CPU å³å¯")
    print("   - ç”Ÿäº§ç¯å¢ƒ: å»ºè®®ä½¿ç”¨ GPU åŠ é€Ÿ")

if __name__ == "__main__":
    main()
